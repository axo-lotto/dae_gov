# Implementation Roadmap: Epoch Training and Testing Infrastructure
## 6-Week Plan for DAE_HYPHAE_1

**Date:** November 13, 2025
**Version:** 1.0
**Status:** Implementation Blueprint

---

## Executive Summary

This roadmap provides a detailed 6-week implementation plan to transform DAE_HYPHAE_1 from its current production-ready state (100% system maturity, single-iteration training) to a comprehensive epoch-based learning system with multi-dimensional testing infrastructure.

**Current State:**
- âœ… 11-organ architecture operational
- âœ… Multi-cycle V0 convergence (Phase 2)
- âœ… DAE 3.0 Fractal Levels 1-4 (R-matrix, Family V0)
- âœ… Arc-inspired training exists
- âš ï¸  No multi-iteration training loop
- âš ï¸  Epoch orchestrator not connected to training
- âš ï¸  No comprehensive testing protocol

**Target State (6 Weeks):**
- âœ… Multi-iteration training with regime adaptation (EXPLORING â†’ COMMITTED)
- âœ… 10-15 epoch training structure with scaffolded difficulty
- âœ… Fractal Levels 5-7 fully operational (Task, Epoch, Global)
- âœ… 27-test comprehensive testing suite (Intelligence, Continuity, Responsiveness, Superject)
- âœ… Automated metrics collection and visualization dashboard
- âœ… Compound learning growth trajectory (target: 40-60% CAGR)

**Deliverables:**
- 15 new source files (~4,000 lines of code)
- 4 YAML configuration files (regime configs)
- 10 test suites with 27 specific tests
- Automated visualization dashboard (HTML + plots)
- Comprehensive analysis reports (markdown)

---

## Table of Contents

1. [Phase A: Epoch Training Infrastructure (Week 1-2)](#phase-a-epoch-training-infrastructure-week-1-2)
2. [Phase B: Testing Infrastructure (Week 3-4)](#phase-b-testing-infrastructure-week-3-4)
3. [Phase C: Metrics Collection + Analysis (Week 5-6)](#phase-c-metrics-collection-analysis-week-5-6)
4. [File Structure and Organization](#file-structure-and-organization)
5. [Integration Points and Dependencies](#integration-points-and-dependencies)
6. [Testing and Validation Strategy](#testing-and-validation-strategy)
7. [Risk Mitigation Plan](#risk-mitigation-plan)
8. [Success Metrics and Checkpoints](#success-metrics-and-checkpoints)

---

## Phase A: Epoch Training Infrastructure (Week 1-2)

### Goal
Implement multi-iteration training with regime-based adaptation and epoch consolidation (Fractal Levels 5-7).

---

### Task A1: Multi-Iteration Trainer (3 days)

**Objective:** Create core multi-iteration training loop with regime adaptation

**File:** `persona_layer/multi_iteration_trainer.py` (~600 lines)

**Implementation Details:**

```python
"""
Multi-Iteration Trainer - DAE 3.0 Regime-Based Training
========================================================

Implements 2-5 iteration training per conversation pair with regime adaptation.

Key Features:
- 4 training regimes (EXPLORING, CONVERGING, STABLE, COMMITTED)
- Adaptive tau threshold (0.3 â†’ 0.75)
- Phase1 entropy scheduling (0.3 â†’ 0.0)
- Hebbian + V0 learning rate schedules

Based on DAE 3.0 multi-iteration training that achieved 62.8% CAGR.
"""

from dataclasses import dataclass
from typing import Dict, List, Optional
import numpy as np


@dataclass
class RegimeConfig:
    """Configuration for a training regime."""
    name: str  # EXPLORING, CONVERGING, STABLE, COMMITTED
    tau_threshold: float  # Emission acceptance threshold
    phase1_entropy: float  # Exploration noise level
    iterations_min: int  # Minimum iterations per pair
    iterations_max: int  # Maximum iterations per pair
    hebbian_learning_rate: float  # R-matrix learning rate
    v0_learning_rate: float  # Family V0 learning rate


class MultiIterationTrainer:
    """
    Multi-iteration training orchestrator.

    Process:
    1. Configure regime (EXPLORING/CONVERGING/STABLE/COMMITTED)
    2. For each training pair:
       - Run 2-5 iterations (regime-dependent)
       - Adapt entropy per iteration (decreasing)
       - Learn from successful emissions (sat â‰¥ tau)
    3. Track task-level success (Râ‚…)
    """

    def __init__(
        self,
        organism_wrapper,
        regime: str = 'EXPLORING',
        custom_config: Optional[RegimeConfig] = None
    ):
        self.organism = organism_wrapper
        self.regime = regime

        # Load regime config
        if custom_config:
            self.config = custom_config
        else:
            self.config = self.get_regime_config(regime)

        print(f"âœ… Multi-Iteration Trainer initialized")
        print(f"   Regime: {self.regime}")
        print(f"   Tau threshold: {self.config.tau_threshold}")
        print(f"   Iterations: {self.config.iterations_min}-{self.config.iterations_max}")
        print(f"   Phase1 entropy: {self.config.phase1_entropy}")

    @staticmethod
    def get_regime_config(regime: str) -> RegimeConfig:
        """Get default configuration for regime."""
        configs = {
            'EXPLORING': RegimeConfig(
                name='EXPLORING',
                tau_threshold=0.30,
                phase1_entropy=0.30,
                iterations_min=2,
                iterations_max=3,
                hebbian_learning_rate=0.08,
                v0_learning_rate=0.15
            ),
            'CONVERGING': RegimeConfig(
                name='CONVERGING',
                tau_threshold=0.50,
                phase1_entropy=0.15,
                iterations_min=3,
                iterations_max=4,
                hebbian_learning_rate=0.05,
                v0_learning_rate=0.10
            ),
            'STABLE': RegimeConfig(
                name='STABLE',
                tau_threshold=0.65,
                phase1_entropy=0.05,
                iterations_min=4,
                iterations_max=5,
                hebbian_learning_rate=0.03,
                v0_learning_rate=0.08
            ),
            'COMMITTED': RegimeConfig(
                name='COMMITTED',
                tau_threshold=0.75,
                phase1_entropy=0.00,
                iterations_min=5,
                iterations_max=5,
                hebbian_learning_rate=0.02,
                v0_learning_rate=0.05
            )
        }

        return configs.get(regime, configs['EXPLORING'])

    def train_on_pair(
        self,
        input_text: str,
        output_text: str,
        pair_metadata: Optional[Dict] = None,
        verbose: bool = False
    ) -> Dict:
        """
        Train on single pair with multiple iterations.

        Returns:
            {
                'iterations_completed': int,
                'successful_iterations': int,
                'mean_satisfaction': float,
                'mean_confidence': float,
                'final_family_id': str,
                'learning_occurred': bool
            }
        """

        iterations_completed = 0
        successful_iterations = 0
        satisfactions = []
        confidences = []
        family_ids = []

        # Determine number of iterations (regime-based)
        num_iterations = np.random.randint(
            self.config.iterations_min,
            self.config.iterations_max + 1
        )

        if verbose:
            print(f"\nðŸ“ Training pair: {num_iterations} iterations")
            print(f"   Tau threshold: {self.config.tau_threshold}")

        for iteration in range(num_iterations):
            # Adapt entropy per iteration (decreases over iterations)
            current_entropy = self.config.phase1_entropy * (1 - iteration / num_iterations)

            # Process conversation
            # NOTE: This requires adding tau_threshold and phase1_entropy parameters
            # to organism.process_text() or setting them via organism config
            result = self.organism.process_text(
                input_text,
                enable_phase2=True,
                enable_tsk_recording=False
            )

            satisfaction = result.get('felt_states', {}).get('satisfaction_final', 0.0)
            confidence = result.get('felt_states', {}).get('emission_confidence', 0.0)
            family_id = result.get('felt_states', {}).get('family_id', None)

            satisfactions.append(satisfaction)
            confidences.append(confidence)
            if family_id:
                family_ids.append(family_id)

            # Check success (satisfies tau threshold)
            success = satisfaction >= self.config.tau_threshold

            if success:
                successful_iterations += 1

                # Learn from successful iteration
                # NOTE: Learning happens automatically in organism's process_text
                # via phase5_learning and organ_coupling_learner hooks

                if verbose:
                    print(f"   âœ… Iteration {iteration+1}/{num_iterations}: "
                          f"sat={satisfaction:.3f}, conf={confidence:.3f}, success")
            else:
                if verbose:
                    print(f"   âŒ Iteration {iteration+1}/{num_iterations}: "
                          f"sat={satisfaction:.3f}, conf={confidence:.3f}, below tau")

            iterations_completed += 1

        return {
            'iterations_completed': iterations_completed,
            'successful_iterations': successful_iterations,
            'mean_satisfaction': np.mean(satisfactions),
            'mean_confidence': np.mean(confidences),
            'final_family_id': family_ids[-1] if family_ids else None,
            'learning_occurred': successful_iterations > 0
        }

    def train_on_corpus(
        self,
        training_pairs: List[Dict],
        verbose: bool = False
    ) -> Dict:
        """
        Train on full corpus with multi-iteration processing.

        Returns:
            {
                'total_pairs': int,
                'total_iterations': int,
                'success_rate': float,
                'mean_satisfaction': float,
                'mean_confidence': float,
                'families_discovered': int
            }
        """

        total_pairs = len(training_pairs)
        total_iterations = 0
        successful_iterations = 0
        all_satisfactions = []
        all_confidences = []
        all_families = set()

        for i, pair in enumerate(training_pairs, 1):
            input_text = pair.get('input_text', pair.get('input', ''))
            output_text = pair.get('output_text', pair.get('output', ''))

            if verbose:
                print(f"\n[{i}/{total_pairs}] Processing pair...")

            result = self.train_on_pair(
                input_text,
                output_text,
                pair_metadata=pair.get('pair_metadata', {}),
                verbose=verbose
            )

            total_iterations += result['iterations_completed']
            successful_iterations += result['successful_iterations']
            all_satisfactions.append(result['mean_satisfaction'])
            all_confidences.append(result['mean_confidence'])

            if result['final_family_id']:
                all_families.add(result['final_family_id'])

        success_rate = successful_iterations / total_iterations if total_iterations > 0 else 0.0

        return {
            'total_pairs': total_pairs,
            'total_iterations': total_iterations,
            'success_rate': success_rate,
            'mean_satisfaction': np.mean(all_satisfactions),
            'mean_confidence': np.mean(all_confidences),
            'families_discovered': len(all_families)
        }
```

**Integration Points:**
- Import `ConversationalOrganismWrapper`
- Call `organism.process_text()` with appropriate parameters
- Access `phase5_learning` and `organ_coupling_learner` for learning validation

**Testing:**
- Run 1 pair with 3 iterations â†’ verify learning occurred
- Run 10 pairs â†’ verify success rate > 0
- Compare EXPLORING vs COMMITTED regime â†’ verify tau adaptation

**Completion Criteria:**
- [ ] File created with all methods implemented
- [ ] 4 regime configs defined and tested
- [ ] Integration with organism wrapper working
- [ ] Test suite passing (10 pairs, 3 iterations each)

---

### Task A2: Epoch Orchestrator Integration (2 days)

**Objective:** Connect `EpochOrchestrator` to training runners for Râ‚…, Râ‚†, Râ‚‡ computation

**File:** `persona_layer/epoch_orchestrator.py` (modify existing, +200 lines)

**Modifications:**

1. **Add training mode integration:**

```python
def train_epoch_with_orchestrator(
    self,
    training_pairs: List[Dict],
    multi_iteration_trainer: 'MultiIterationTrainer',
    verbose: bool = False
) -> EpochResult:
    """
    Train full epoch with task tracking and consolidation.

    Integration with MultiIterationTrainer for Râ‚…, Râ‚†, Râ‚‡.
    """

    for pair in training_pairs:
        # Train on pair with multi-iteration
        pair_result = multi_iteration_trainer.train_on_pair(
            input_text=pair['input_text'],
            output_text=pair['output_text'],
            verbose=verbose
        )

        # Create TaskResult (Râ‚…)
        task_result = TaskResult(
            task_id=f"task_{self.global_state.total_tasks + len(self.current_epoch_tasks) + 1:05d}",
            input_text=pair['input_text'],
            emission_text="",  # Not stored at task level
            satisfaction=pair_result['mean_satisfaction'],
            confidence=pair_result['mean_confidence'],
            family_id=pair_result['final_family_id'],
            v0_final=0.0,  # Average across iterations
            convergence_cycles=0,  # Average across iterations
            success=pair_result['mean_satisfaction'] >= multi_iteration_trainer.config.tau_threshold,
            timestamp=datetime.now().isoformat()
        )

        self.current_epoch_tasks.append(task_result)

    # Consolidate epoch (Râ‚†) and update global (Râ‚‡)
    epoch_result = self._consolidate_epoch()

    return epoch_result
```

2. **Add regime tracking to EpochResult:**

```python
@dataclass
class EpochResult:
    # ... existing fields ...
    regime: str  # NEW: EXPLORING, CONVERGING, STABLE, COMMITTED
    tau_threshold: float  # NEW: regime-specific threshold
```

**Completion Criteria:**
- [ ] `train_epoch_with_orchestrator()` method added
- [ ] Râ‚…, Râ‚†, Râ‚‡ computed and saved
- [ ] Regime tracked in epoch results
- [ ] Test: Run 1 epoch â†’ verify Râ‚†, Râ‚‡ saved to JSON

---

### Task A3: Training Runner Scripts (2 days)

**Objective:** Create main training scripts with corpus scaffolding

**File 1:** `training/conversational/run_epoch_training.py` (~400 lines)

```python
#!/usr/bin/env python3
"""
Epoch Training Runner - Full 10-15 Epoch Training
==================================================

Runs complete epoch-based training with:
- Corpus scaffolding (difficulty progression)
- Regime adaptation (EXPLORING â†’ COMMITTED)
- Multi-iteration processing (2-5 iterations per pair)
- Epoch consolidation (Râ‚…, Râ‚†, Râ‚‡)
- Results persistence

Usage:
    python3 training/conversational/run_epoch_training.py --num-epochs 15
"""

import argparse
import json
from pathlib import Path
from persona_layer.conversational_organism_wrapper import ConversationalOrganismWrapper
from persona_layer.multi_iteration_trainer import MultiIterationTrainer
from persona_layer.epoch_orchestrator import EpochOrchestrator


def load_training_corpus(corpus_path: str) -> List[Dict]:
    """Load full training corpus (200 pairs)."""
    with open(corpus_path, 'r') as f:
        data = json.load(f)
    return data.get('training_pairs', [])


def scaffold_corpus(
    training_pairs: List[Dict],
    epoch_id: int,
    total_epochs: int = 15
) -> List[Dict]:
    """
    Select training pairs for this epoch with difficulty scaffolding.

    Epochs 1-3 (EXPLORING): Simple pairs (casual, grounding)
    Epochs 4-7 (CONVERGING): Moderate pairs (transduction, burnout)
    Epochs 8-10 (STABLE): Complex pairs (multi-mechanism, deep relational)
    Epochs 11-15 (COMMITTED): Transfer pairs (novel contexts, meta-cognitive)
    """

    # Categorize pairs by difficulty
    simple_categories = ['casual', 'grounding', 'gratitude', 'appreciation']
    moderate_categories = ['burnout', 'productivity', 'boundaries', 'transduction']
    complex_categories = ['trauma', 'fragmented', 'parts_work', 'scapegoat']
    transfer_categories = ['novel', 'meta_cognitive', 'self_awareness']

    def get_category(pair):
        return pair.get('pair_metadata', {}).get('category', pair.get('category', 'unknown'))

    simple_pairs = [p for p in training_pairs if get_category(p) in simple_categories]
    moderate_pairs = [p for p in training_pairs if get_category(p) in moderate_categories]
    complex_pairs = [p for p in training_pairs if get_category(p) in complex_categories]
    transfer_pairs = [p for p in training_pairs if get_category(p) in transfer_categories]

    # Epoch-based selection
    if epoch_id <= 3:
        # EXPLORING: 30 simple pairs
        return np.random.choice(simple_pairs, min(30, len(simple_pairs)), replace=False).tolist()
    elif epoch_id <= 7:
        # CONVERGING: 50 moderate pairs
        return np.random.choice(moderate_pairs, min(50, len(moderate_pairs)), replace=False).tolist()
    elif epoch_id <= 10:
        # STABLE: 60 complex pairs
        return np.random.choice(complex_pairs, min(60, len(complex_pairs)), replace=False).tolist()
    else:
        # COMMITTED: 40 transfer pairs
        return np.random.choice(transfer_pairs, min(40, len(transfer_pairs)), replace=False).tolist()


def get_regime_for_epoch(epoch_id: int) -> str:
    """Map epoch to training regime."""
    if epoch_id <= 3:
        return 'EXPLORING'
    elif epoch_id <= 7:
        return 'CONVERGING'
    elif epoch_id <= 10:
        return 'STABLE'
    else:
        return 'COMMITTED'


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--num-epochs', type=int, default=15)
    parser.add_argument('--corpus-path', type=str,
                        default='knowledge_base/conversational_training_pairs_complete.json')
    parser.add_argument('--results-dir', type=str, default='results/epochs')
    parser.add_argument('--verbose', action='store_true')
    args = parser.parse_args()

    print("\n" + "="*80)
    print("ðŸŒ€ EPOCH TRAINING - DAE_HYPHAE_1")
    print("="*80 + "\n")

    # Load corpus
    training_pairs = load_training_corpus(args.corpus_path)
    print(f"ðŸ“š Loaded {len(training_pairs)} training pairs\n")

    # Initialize organism
    organism = ConversationalOrganismWrapper()

    # Initialize epoch orchestrator
    orchestrator = EpochOrchestrator(
        organism_wrapper=organism,
        epoch_size=50,  # Will be dynamically adjusted
        results_dir=Path(args.results_dir)
    )

    # Training loop
    for epoch_id in range(1, args.num_epochs + 1):
        print(f"\n{'='*80}")
        print(f"EPOCH {epoch_id}/{args.num_epochs}")
        print(f"{'='*80}\n")

        # Determine regime
        regime = get_regime_for_epoch(epoch_id)

        # Scaffold corpus for this epoch
        epoch_pairs = scaffold_corpus(training_pairs, epoch_id, args.num_epochs)

        print(f"ðŸ“‹ Regime: {regime}")
        print(f"ðŸ“‹ Pairs for this epoch: {len(epoch_pairs)}\n")

        # Initialize multi-iteration trainer for this regime
        trainer = MultiIterationTrainer(organism, regime=regime)

        # Train epoch
        epoch_result = orchestrator.train_epoch_with_orchestrator(
            training_pairs=epoch_pairs,
            multi_iteration_trainer=trainer,
            verbose=args.verbose
        )

        print(f"\nðŸ“Š Epoch {epoch_id} Complete:")
        print(f"   Success rate: {epoch_result.success_rate:.1%}")
        print(f"   Mean satisfaction: {epoch_result.mean_satisfaction:.3f}")
        print(f"   Epoch reward (Râ‚†): {epoch_result.epoch_reward:.3f}")
        print(f"   Global confidence (Râ‚‡): {orchestrator.global_state.global_confidence:.3f}")

    print(f"\n{'='*80}")
    print(f"ðŸŽ‰ TRAINING COMPLETE - {args.num_epochs} EPOCHS")
    print(f"{'='*80}\n")

    # Final report
    print(f"ðŸ“Š Final Global State:")
    print(f"   Total tasks: {orchestrator.global_state.total_tasks}")
    print(f"   Global confidence (Râ‚‡): {orchestrator.global_state.global_confidence:.3f}")
    print(f"   Compound growth rate: {orchestrator.global_state.compound_growth_rate:+.1%} per epoch")


if __name__ == '__main__':
    main()
```

**Completion Criteria:**
- [ ] Script runs full 15-epoch training
- [ ] Corpus scaffolding working (simple â†’ complex)
- [ ] Regime transitions correctly (EXPLORING â†’ COMMITTED)
- [ ] Results saved to `results/epochs/`
- [ ] Global confidence (Râ‚‡) computed

---

### Task A4: Regime Adaptation Mechanism (2 days)

**Objective:** Implement adaptive tau threshold and entropy scheduling

**File:** `persona_layer/regime_adapter.py` (~200 lines)

```python
"""
Regime Adapter - Dynamic Training Regime Management
====================================================

Manages regime transitions and adaptive parameter scheduling.

Features:
- Tau threshold scheduling (0.3 â†’ 0.75)
- Phase1 entropy scheduling (0.3 â†’ 0.0)
- Learning rate scheduling (hebbian + v0)
- Automatic regime detection based on success rate
"""

class RegimeAdapter:
    """
    Adaptive regime management with automatic transitions.

    Monitors training progress and adjusts regime dynamically.
    """

    def __init__(self, initial_regime: str = 'EXPLORING'):
        self.current_regime = initial_regime
        self.regime_history = [initial_regime]
        self.epoch_counter = 0

    def suggest_next_regime(
        self,
        current_success_rate: float,
        epochs_in_regime: int
    ) -> str:
        """
        Suggest regime transition based on performance.

        Rules:
        - Stay in EXPLORING if success_rate < 0.70 (need more exploration)
        - Transition to CONVERGING if success_rate â‰¥ 0.70 for 2+ epochs
        - Transition to STABLE if success_rate â‰¥ 0.75 for 2+ epochs
        - Transition to COMMITTED if success_rate â‰¥ 0.80 for 2+ epochs
        """

        if self.current_regime == 'EXPLORING':
            if current_success_rate >= 0.70 and epochs_in_regime >= 2:
                return 'CONVERGING'

        elif self.current_regime == 'CONVERGING':
            if current_success_rate >= 0.75 and epochs_in_regime >= 2:
                return 'STABLE'
            elif current_success_rate < 0.60:
                # Regression - go back to EXPLORING
                return 'EXPLORING'

        elif self.current_regime == 'STABLE':
            if current_success_rate >= 0.80 and epochs_in_regime >= 2:
                return 'COMMITTED'
            elif current_success_rate < 0.65:
                # Regression - go back to CONVERGING
                return 'CONVERGING'

        elif self.current_regime == 'COMMITTED':
            if current_success_rate < 0.70:
                # Regression - go back to STABLE
                return 'STABLE'

        # Default: stay in current regime
        return self.current_regime

    def adaptive_tau(self, success_rate: float) -> float:
        """
        Adaptive tau threshold based on current success rate.

        If success rate drops below 50%, lower tau temporarily.
        """

        base_tau = MultiIterationTrainer.get_regime_config(self.current_regime).tau_threshold

        if success_rate < 0.50:
            # Lower tau to prevent emission failure
            return max(base_tau - 0.10, 0.20)

        return base_tau
```

**Completion Criteria:**
- [ ] Regime transition logic implemented
- [ ] Adaptive tau threshold working
- [ ] Integration with training runner
- [ ] Test: Success rate < 0.50 â†’ tau automatically lowered

---

### Week 1-2 Deliverables Summary

**Files Created:**
1. `persona_layer/multi_iteration_trainer.py` (~600 lines)
2. `training/conversational/run_epoch_training.py` (~400 lines)
3. `persona_layer/regime_adapter.py` (~200 lines)

**Files Modified:**
1. `persona_layer/epoch_orchestrator.py` (+200 lines)

**Total Code:** ~1,400 lines

**Testing:**
- [ ] Run 1-epoch pilot (30 pairs, 2-3 iterations) â†’ verify infrastructure
- [ ] Run 5-epoch test (EXPLORING â†’ CONVERGING transition) â†’ verify regime adaptation
- [ ] Verify Râ‚…, Râ‚†, Râ‚‡ computed and saved

---

## Phase B: Testing Infrastructure (Week 3-4)

### Goal
Implement 4-dimensional testing protocol (Intelligence, Continuity, Responsiveness, Superject) with 27 specific tests.

---

### Task B1: Intelligence Test Suite (3 days)

**Objective:** Implement 5 intelligence tests (pattern completion, abstraction, transfer)

**Files:**
1. `tests/intelligence/test_pattern_completion.py` (~200 lines)
2. `tests/intelligence/test_cross_category_transfer.py` (~200 lines)
3. `tests/intelligence/test_meta_pattern_recognition.py` (~150 lines)
4. `tests/intelligence/test_metacognitive_awareness.py` (~150 lines)
5. `tests/intelligence/test_novel_context_generalization.py` (~200 lines)

**Test Infrastructure:**

```python
# tests/intelligence/intelligence_test_base.py
"""
Base class for intelligence tests with shared utilities.
"""

class IntelligenceTestBase:
    """Base class with common intelligence test utilities."""

    def __init__(self, organism):
        self.organism = organism
        self.embedder = self._load_embedder()

    def _load_embedder(self):
        """Load SANS embedding model for semantic similarity."""
        from sentence_transformers import SentenceTransformer
        return SentenceTransformer('all-MiniLM-L6-v2')

    def compute_semantic_similarity(self, text1: str, text2: str) -> float:
        """Compute cosine similarity between two texts."""
        emb1 = self.embedder.encode(text1, convert_to_numpy=True)
        emb2 = self.embedder.encode(text2, convert_to_numpy=True)

        # Normalize
        emb1_norm = emb1 / np.linalg.norm(emb1)
        emb2_norm = emb2 / np.linalg.norm(emb2)

        # Cosine similarity
        similarity = float(np.dot(emb1_norm, emb2_norm))

        # Normalize to [0, 1]
        return (similarity + 1.0) / 2.0

    def select_random_triplet(self, category: str, training_pairs: List[Dict]):
        """Select 3 random pairs from category for arc testing."""
        category_pairs = [
            p for p in training_pairs
            if p.get('pair_metadata', {}).get('category', '') == category
        ]

        if len(category_pairs) < 3:
            return None

        indices = np.random.choice(len(category_pairs), 3, replace=False)
        return category_pairs[indices[0]], category_pairs[indices[1]], category_pairs[indices[2]]
```

**Completion Criteria:**
- [ ] 5 intelligence test files created
- [ ] Shared base class with semantic similarity
- [ ] All tests return structured results dict
- [ ] Test suite runnable: `python3 tests/intelligence/run_all.py`

---

### Task B2: Continuity Test Suite (3 days)

**Objective:** Implement 7 continuity tests (Yâ†’Xâ†’Zâ†’X' validation)

**Files:**
1. `tests/continuity/test_hebbian_prehension.py` (~150 lines)
2. `tests/continuity/test_v0_target_guidance.py` (~150 lines)
3. `tests/continuity/test_family_stability.py` (~100 lines)
4. `tests/continuity/test_satisfaction_emission_coherence.py` (~150 lines)
5. `tests/continuity/test_v0_descent_quality.py` (~100 lines)
6. `tests/continuity/test_cross_conversation_consistency.py` (~150 lines)
7. `tests/continuity/test_hebbian_memory_growth.py` (~100 lines)

**Key Implementation:**

```python
# tests/continuity/test_hebbian_prehension.py
"""
Test Yâ†’X Prehension: Hebbian R-Matrix Influence
================================================

Validates that learned R-matrix couplings influence novel organ activations.
"""

def test_hebbian_prehension(organism, training_set, novel_input):
    """
    Test if learned R-matrix couplings predict novel co-activations.

    Process:
    1. Train on trauma triad inputs (BOND+EO+NDAM co-activate)
    2. Capture R-matrix state
    3. Process novel trauma input
    4. Correlation: R-matrix couplings vs observed co-activations
    """

    # Step 1: Train on trauma inputs
    for pair in training_set['trauma_triad']:
        organism.process_text(pair['input_text'])

    # Step 2: Extract R-matrix
    R_matrix = organism.organ_coupling_learner.R_matrix.copy()

    # Step 3: Process novel input
    result = organism.process_text(novel_input)

    # Step 4: Compute correlation
    all_couplings = []
    all_coactivations = []

    for i in range(11):
        for j in range(i+1, 11):
            all_couplings.append(R_matrix[i, j])
            coh_i = result['organ_coherences'][organism.organ_names[i]]
            coh_j = result['organ_coherences'][organism.organ_names[j]]
            all_coactivations.append(coh_i * coh_j)

    correlation = np.corrcoef(all_couplings, all_coactivations)[0, 1]

    return {
        'test_id': 'CONT-001',
        'test_name': 'Hebbian R-Matrix Prehension',
        'correlation': correlation,
        'threshold': 0.60,
        'passed': correlation >= 0.60
    }
```

**Completion Criteria:**
- [ ] 7 continuity test files created
- [ ] All tests measure Yâ†’X, Xâ†’Z, or Zâ†’X' phases
- [ ] Baseline capture functionality (save initial state)
- [ ] Test suite runnable: `python3 tests/continuity/run_all.py`

---

### Task B3: Responsiveness Test Suite (2 days)

**Objective:** Implement 6 responsiveness tests (speed, quality, adaptation)

**Files:**
1. `tests/responsiveness/test_processing_time_stability.py` (~100 lines)
2. `tests/responsiveness/test_convergence_efficiency.py` (~100 lines)
3. `tests/responsiveness/test_satisfaction_calibration.py` (~150 lines)
4. `tests/responsiveness/test_pathway_accuracy.py` (~150 lines)
5. `tests/responsiveness/test_regime_adaptation.py` (~100 lines)
6. `tests/responsiveness/test_family_discovery_rate.py` (~100 lines)

**Completion Criteria:**
- [ ] 6 responsiveness test files created
- [ ] Longitudinal tracking (epoch-to-epoch comparison)
- [ ] Performance profiling (time measurements accurate)
- [ ] Test suite runnable: `python3 tests/responsiveness/run_all.py`

---

### Task B4: Superject Cycle Test (3 days)

**Objective:** Implement integrated Xâ†’Yâ†’Zâ†’X' cycle test

**File:** `tests/superject/test_complete_cycle.py` (~400 lines)

**Implementation:**

```python
"""
Complete Whiteheadian Superject Cycle Test
==========================================

Validates full Xâ†’Yâ†’Zâ†’X' process philosophy implementation.

Phases:
1. X prehends Y (current accesses past via R-matrix, families)
2. X achieves satisfaction â†’ Z (concrescence produces emission)
3. Z objectifies (emission becomes data for future via learning)
4. X' prehends Z (next conversation influenced by previous)
"""

def test_complete_superject_cycle(organism, training_sequence):
    """
    Run complete 4-phase superject cycle test.

    Args:
        organism: ConversationalOrganismWrapper
        training_sequence: List of 2+ related conversation pairs

    Returns:
        {
            'X_prehends_Y': {'prehension_quality': float, 'success': bool},
            'X_achieves_Z': {'satisfaction': float, 'emission_quality': float, 'success': bool},
            'Z_objectifies': {'r_matrix_updated': bool, 'families_updated': bool, 'success': bool},
            'X_prime_prehends_Z': {'learning_influence': bool, 'success': bool},
            'superject_cycle_complete': bool
        }
    """

    # PHASE 1: X prehends Y
    R_matrix_before = organism.organ_coupling_learner.R_matrix.copy()
    result_1 = organism.process_text(training_sequence[0]['input_text'])

    prehension_quality = compute_coupling_activation_correlation(
        R_matrix_before,
        result_1['organ_coherences']
    )

    phase1_success = prehension_quality >= 0.60

    # PHASE 2: X achieves satisfaction â†’ Z
    satisfaction = result_1['felt_states']['satisfaction_final']
    emission_quality = compute_semantic_similarity(
        result_1['emission_text'],
        training_sequence[0]['output_text']
    )

    phase2_success = (satisfaction >= 0.75 and emission_quality >= 0.70)

    # PHASE 3: Z objectifies
    organism.phase5_learning.learn_from_conversation(
        organ_results=result_1['organ_results'],
        assembled_response=result_1['assembled_response'],
        user_message=training_sequence[0]['input_text']
    )

    R_matrix_after = organism.organ_coupling_learner.R_matrix.copy()

    r_matrix_updated = not np.array_equal(R_matrix_before, R_matrix_after)
    families_updated = True  # Check family count or state change

    phase3_success = r_matrix_updated and families_updated

    # PHASE 4: X' prehends Z
    result_2 = organism.process_text(training_sequence[1]['input_text'])

    post_learning_correlation = compute_coupling_activation_correlation(
        R_matrix_after,
        result_2['organ_coherences']
    )

    learning_influence = post_learning_correlation > prehension_quality
    phase4_success = learning_influence

    # Aggregate
    return {
        'X_prehends_Y': {
            'prehension_quality': prehension_quality,
            'success': phase1_success
        },
        'X_achieves_Z': {
            'satisfaction': satisfaction,
            'emission_quality': emission_quality,
            'success': phase2_success
        },
        'Z_objectifies': {
            'r_matrix_updated': r_matrix_updated,
            'families_updated': families_updated,
            'success': phase3_success
        },
        'X_prime_prehends_Z': {
            'post_learning_correlation': post_learning_correlation,
            'learning_influence': learning_influence,
            'success': phase4_success
        },
        'superject_cycle_complete': all([
            phase1_success, phase2_success, phase3_success, phase4_success
        ])
    }
```

**Completion Criteria:**
- [ ] Complete cycle test implemented
- [ ] All 4 phases validated
- [ ] Test runnable: `python3 tests/superject/test_complete_cycle.py`
- [ ] Benchmark thresholds enforced (50% epoch 1-3, 70% epoch 4-7, 85% epoch 8-15)

---

### Week 3-4 Deliverables Summary

**Files Created:**
- Intelligence tests: 6 files (~900 lines)
- Continuity tests: 7 files (~900 lines)
- Responsiveness tests: 6 files (~700 lines)
- Superject test: 1 file (~400 lines)

**Total Code:** ~2,900 lines

**Testing:**
- [ ] Run baseline tests (epoch 0) â†’ establish floor
- [ ] Run periodic tests (epoch 5) â†’ verify learning
- [ ] Run final tests (epoch 15) â†’ assess success level

---

## Phase C: Metrics Collection + Analysis (Week 5-6)

### Goal
Automated metrics collection, visualization dashboard, and analysis reports.

---

### Task C1: Metrics Collector (2 days)

**Objective:** Auto-collect metrics during training + testing

**File:** `monitoring/epoch_metrics_collector.py` (~300 lines)

```python
"""
Epoch Metrics Collector - Automated Metrics Collection
=======================================================

Collects metrics during training and testing:
- Training metrics: Râ‚…, Râ‚†, Râ‚‡, success rate, satisfaction
- Intelligence metrics: Pattern completion, transfer, meta-cognitive
- Continuity metrics: R-matrix growth, family stability, Yâ†’Xâ†’Z
- Responsiveness metrics: Processing time, convergence efficiency
"""

class EpochMetricsCollector:
    """
    Automated metrics collection for epoch training.

    Integrates with:
    - MultiIterationTrainer (training metrics)
    - EpochOrchestrator (Râ‚…, Râ‚†, Râ‚‡)
    - Intelligence tests (periodic)
    - Continuity tests (periodic)
    """

    def __init__(self, results_dir: Path = Path('results/metrics')):
        self.results_dir = results_dir
        self.results_dir.mkdir(parents=True, exist_ok=True)

        self.metrics_history = {
            'training': [],
            'intelligence': [],
            'continuity': [],
            'responsiveness': [],
            'superject': []
        }

    def collect_training_metrics(self, epoch_id: int, epoch_result: EpochResult):
        """Collect metrics from completed epoch."""
        self.metrics_history['training'].append({
            'epoch_id': epoch_id,
            'success_rate': epoch_result.success_rate,
            'mean_satisfaction': epoch_result.mean_satisfaction,
            'mean_confidence': epoch_result.mean_confidence,
            'epoch_reward': epoch_result.epoch_reward,
            'global_confidence': epoch_result.global_confidence,  # Râ‚‡
            'families_discovered': epoch_result.families_discovered,
            'timestamp': epoch_result.timestamp
        })

    def run_intelligence_tests(self, organism, epoch_id: int):
        """Run intelligence test suite and collect results."""
        # Import test suite
        from tests.intelligence import run_all_intelligence_tests

        results = run_all_intelligence_tests(organism)

        self.metrics_history['intelligence'].append({
            'epoch_id': epoch_id,
            'results': results,
            'aggregate_score': results['intelligence_score'],
            'timestamp': datetime.now().isoformat()
        })

    def save_metrics(self):
        """Save metrics history to JSON."""
        output_path = self.results_dir / 'metrics_history.json'

        with open(output_path, 'w') as f:
            json.dump(self.metrics_history, f, indent=2)
```

**Completion Criteria:**
- [ ] Metrics collector created
- [ ] Integration with training runner
- [ ] Auto-save to JSON after each epoch
- [ ] Test: Run 3 epochs â†’ verify metrics saved

---

### Task C2: Visualization Dashboard (3 days)

**Objective:** Interactive dashboard for epoch metrics

**File:** `visualization/epoch_dashboard.py` (~500 lines)

**Features:**
- Epoch-level plots (Râ‚† trajectory, Râ‚‡ global confidence, CAGR)
- Intelligence plots (pattern completion over time, transfer accuracy)
- Continuity plots (R-matrix growth heatmap, family stability)
- Responsiveness plots (processing time, convergence efficiency)

**Example Plot:**

```python
def plot_global_confidence_trajectory(metrics_history):
    """
    Plot Râ‚‡ (global confidence) over epochs.

    Shows:
    - Râ‚‡ trajectory
    - Râ‚† (epoch rewards) as markers
    - CAGR trendline
    """

    import matplotlib.pyplot as plt

    epochs = [m['epoch_id'] for m in metrics_history['training']]
    r7_values = [m['global_confidence'] for m in metrics_history['training']]
    r6_values = [m['epoch_reward'] for m in metrics_history['training']]

    fig, ax = plt.subplots(figsize=(10, 6))

    # Râ‚‡ trajectory
    ax.plot(epochs, r7_values, 'b-', linewidth=2, label='Global Confidence (Râ‚‡)')

    # Râ‚† markers
    ax.scatter(epochs, r6_values, c='orange', s=50, alpha=0.6, label='Epoch Reward (Râ‚†)')

    # CAGR trendline
    if len(r7_values) >= 2:
        from scipy.stats import linregress
        slope, intercept, _, _, _ = linregress(epochs, np.log(r7_values))
        cagr = (np.exp(slope) - 1) * 100
        trendline = np.exp(intercept + slope * np.array(epochs))

        ax.plot(epochs, trendline, 'r--', alpha=0.5,
                label=f'CAGR: {cagr:+.1f}% per epoch')

    ax.set_xlabel('Epoch')
    ax.set_ylabel('Confidence / Reward')
    ax.set_title('Global Learning Trajectory (Râ‚‡)')
    ax.legend()
    ax.grid(True, alpha=0.3)

    return fig
```

**Completion Criteria:**
- [ ] Dashboard script created
- [ ] 10+ plot types implemented
- [ ] HTML export with interactive plots
- [ ] Test: Generate dashboard from 15-epoch results

---

### Task C3: Analysis Reports (2 days)

**Objective:** Automated markdown analysis reports

**File:** `analysis/generate_epoch_report.py` (~400 lines)

**Report Sections:**
1. Executive Summary (system maturity level, CAGR)
2. Training Metrics (success rate, satisfaction, confidence)
3. Intelligence Assessment (aggregate score, level classification)
4. Continuity Assessment (Yâ†’Xâ†’Z cycle health)
5. Responsiveness Assessment (speed, quality, adaptation)
6. Benchmark Comparison (target vs actual metrics)
7. Failure Mode Analysis (tests that failed, diagnosis)
8. Recommendations (next steps based on results)

**Example:**

```python
def generate_executive_summary(metrics_history):
    """
    Generate executive summary section.

    Includes:
    - Final system maturity level (Functional/Operational/Excellent/Exceptional)
    - CAGR
    - Key achievements
    - Critical issues
    """

    final_epoch = metrics_history['training'][-1]
    intelligence = metrics_history['intelligence'][-1]
    continuity = metrics_history['continuity'][-1]

    # Compute maturity level
    tests_passed = (
        sum([r['passed'] for r in intelligence['results']]) +
        sum([r['passed'] for r in continuity['results']])
    )
    total_tests = len(intelligence['results']) + len(continuity['results'])
    pass_rate = tests_passed / total_tests

    if pass_rate >= 0.85:
        maturity_level = 'Excellent (Research Quality)'
    elif pass_rate >= 0.70:
        maturity_level = 'Operational (Production Ready)'
    elif pass_rate >= 0.50:
        maturity_level = 'Functional (Minimum Viable)'
    else:
        maturity_level = 'Development (Not Production Ready)'

    # CAGR
    cagr = final_epoch.get('compound_growth_rate', 0.0)

    summary = f"""
# Executive Summary

**System Maturity Level:** {maturity_level}

**Overall Performance:**
- Tests Passed: {tests_passed}/{total_tests} ({pass_rate:.1%})
- Global Confidence (Râ‚‡): {final_epoch['global_confidence']:.3f}
- Compound Growth Rate: {cagr:+.1%} per epoch
- Intelligence Score: {intelligence['aggregate_score']:.3f}
- Continuity Score: {continuity['aggregate_score']:.3f}

**Key Achievements:**
- âœ… Multi-iteration training operational ({final_epoch['epoch_id']} epochs completed)
- âœ… Fractal Levels 1-7 functional (Hebbian â†’ Global confidence)
- âœ… Organic family discovery ({final_epoch['families_discovered']} families)

**Critical Issues:**
{generate_critical_issues_list(metrics_history)}

**Recommendation:** {generate_recommendation(maturity_level, cagr)}
"""

    return summary
```

**Completion Criteria:**
- [ ] Report generator created
- [ ] All 8 sections implemented
- [ ] Markdown output with tables and plots
- [ ] Test: Generate report from 15-epoch training

---

### Task C4: Experiment Tracking (2 days)

**Objective:** A/B testing framework for different configurations

**File:** `experiments/experiment_manager.py` (~300 lines)

```python
"""
Experiment Manager - A/B Testing and Configuration Management
==============================================================

Features:
- Experiment configuration versioning
- Result reproducibility (seed management)
- A/B comparison (different regime configs, learning rates)
- Statistical significance testing
"""

class ExperimentManager:
    """
    Manage multiple training experiments with different configs.

    Supports:
    - Baseline experiment (default config)
    - Variant experiments (modified configs)
    - Statistical comparison (t-tests)
    """

    def __init__(self, experiments_dir: Path = Path('experiments')):
        self.experiments_dir = experiments_dir
        self.experiments_dir.mkdir(parents=True, exist_ok=True)

    def create_experiment(
        self,
        name: str,
        config: Dict,
        seed: int = 42
    ) -> str:
        """
        Create new experiment with configuration.

        Returns:
            experiment_id: Unique identifier
        """

        experiment_id = f"{name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        experiment_dir = self.experiments_dir / experiment_id
        experiment_dir.mkdir(parents=True, exist_ok=True)

        # Save config
        config_path = experiment_dir / 'config.json'
        with open(config_path, 'w') as f:
            json.dump({
                'name': name,
                'config': config,
                'seed': seed,
                'created': datetime.now().isoformat()
            }, f, indent=2)

        return experiment_id

    def compare_experiments(
        self,
        baseline_id: str,
        variant_ids: List[str],
        metric: str = 'global_confidence'
    ) -> Dict:
        """
        Compare baseline vs variants on specified metric.

        Returns statistical comparison results.
        """

        from scipy.stats import ttest_ind

        # Load baseline
        baseline_metrics = self.load_experiment_metrics(baseline_id)
        baseline_values = [m[metric] for m in baseline_metrics['training']]

        results = {'baseline': baseline_id, 'comparisons': []}

        for variant_id in variant_ids:
            variant_metrics = self.load_experiment_metrics(variant_id)
            variant_values = [m[metric] for m in variant_metrics['training']]

            # T-test
            t_stat, p_value = ttest_ind(baseline_values, variant_values)

            # Mean improvement
            baseline_mean = np.mean(baseline_values)
            variant_mean = np.mean(variant_values)
            improvement = (variant_mean - baseline_mean) / baseline_mean

            results['comparisons'].append({
                'variant_id': variant_id,
                't_statistic': t_stat,
                'p_value': p_value,
                'significant': p_value < 0.05,
                'baseline_mean': baseline_mean,
                'variant_mean': variant_mean,
                'improvement': improvement
            })

        return results
```

**Completion Criteria:**
- [ ] Experiment manager created
- [ ] Configuration versioning working
- [ ] A/B comparison with statistical tests
- [ ] Test: Run 2 experiments with different configs â†’ compare results

---

### Week 5-6 Deliverables Summary

**Files Created:**
- `monitoring/epoch_metrics_collector.py` (~300 lines)
- `visualization/epoch_dashboard.py` (~500 lines)
- `analysis/generate_epoch_report.py` (~400 lines)
- `experiments/experiment_manager.py` (~300 lines)

**Total Code:** ~1,500 lines

**Testing:**
- [ ] Collect metrics from 15-epoch training
- [ ] Generate visualization dashboard
- [ ] Generate analysis report
- [ ] Run A/B test (baseline vs learning rate variant)

---

## File Structure and Organization

### Final Directory Structure

```
DAE_HYPHAE_1/
â”œâ”€â”€ persona_layer/
â”‚   â”œâ”€â”€ multi_iteration_trainer.py           # NEW (A1)
â”‚   â”œâ”€â”€ regime_adapter.py                    # NEW (A4)
â”‚   â”œâ”€â”€ epoch_orchestrator.py                # MODIFIED (A2)
â”‚   â””â”€â”€ [existing files]
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ conversational/
â”‚   â”‚   â”œâ”€â”€ run_epoch_training.py            # NEW (A3)
â”‚   â”‚   â”œâ”€â”€ run_baseline_training.py         # EXISTING
â”‚   â”‚   â””â”€â”€ [configs/]                       # NEW (YAML configs)
â”‚   â””â”€â”€ [other training scripts]
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ intelligence/                        # NEW (B1)
â”‚   â”‚   â”œâ”€â”€ intelligence_test_base.py
â”‚   â”‚   â”œâ”€â”€ test_pattern_completion.py
â”‚   â”‚   â”œâ”€â”€ test_cross_category_transfer.py
â”‚   â”‚   â”œâ”€â”€ test_meta_pattern_recognition.py
â”‚   â”‚   â”œâ”€â”€ test_metacognitive_awareness.py
â”‚   â”‚   â”œâ”€â”€ test_novel_context_generalization.py
â”‚   â”‚   â””â”€â”€ run_all.py
â”‚   â”œâ”€â”€ continuity/                          # NEW (B2)
â”‚   â”‚   â”œâ”€â”€ test_hebbian_prehension.py
â”‚   â”‚   â”œâ”€â”€ test_v0_target_guidance.py
â”‚   â”‚   â”œâ”€â”€ test_family_stability.py
â”‚   â”‚   â”œâ”€â”€ test_satisfaction_emission_coherence.py
â”‚   â”‚   â”œâ”€â”€ test_v0_descent_quality.py
â”‚   â”‚   â”œâ”€â”€ test_cross_conversation_consistency.py
â”‚   â”‚   â”œâ”€â”€ test_hebbian_memory_growth.py
â”‚   â”‚   â””â”€â”€ run_all.py
â”‚   â”œâ”€â”€ responsiveness/                      # NEW (B3)
â”‚   â”‚   â”œâ”€â”€ test_processing_time_stability.py
â”‚   â”‚   â”œâ”€â”€ test_convergence_efficiency.py
â”‚   â”‚   â”œâ”€â”€ test_satisfaction_calibration.py
â”‚   â”‚   â”œâ”€â”€ test_pathway_accuracy.py
â”‚   â”‚   â”œâ”€â”€ test_regime_adaptation.py
â”‚   â”‚   â”œâ”€â”€ test_family_discovery_rate.py
â”‚   â”‚   â””â”€â”€ run_all.py
â”‚   â”œâ”€â”€ superject/                           # NEW (B4)
â”‚   â”‚   â””â”€â”€ test_complete_cycle.py
â”‚   â””â”€â”€ [existing test directories]
â”‚
â”œâ”€â”€ monitoring/
â”‚   â””â”€â”€ epoch_metrics_collector.py           # NEW (C1)
â”‚
â”œâ”€â”€ visualization/
â”‚   â””â”€â”€ epoch_dashboard.py                   # NEW (C2)
â”‚
â”œâ”€â”€ analysis/
â”‚   â””â”€â”€ generate_epoch_report.py             # NEW (C3)
â”‚
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ experiment_manager.py                # NEW (C4)
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ epochs/                              # Epoch training results
â”‚   â”œâ”€â”€ metrics/                             # Collected metrics
â”‚   â”œâ”€â”€ visualizations/                      # Dashboard exports
â”‚   â””â”€â”€ reports/                             # Analysis reports
â”‚
â””â”€â”€ [existing files]
```

---

## Integration Points and Dependencies

### External Dependencies

**Python Packages (add to requirements.txt):**
```
sentence-transformers>=2.2.0  # Semantic similarity (SANS embeddings)
scipy>=1.9.0                  # Statistical tests, linear regression
matplotlib>=3.5.0             # Visualization
seaborn>=0.12.0               # Statistical plots
pyyaml>=6.0                   # YAML config loading
```

### Internal Dependencies

**Phase A depends on:**
- `ConversationalOrganismWrapper` (existing)
- `EpochOrchestrator` (existing, to be modified)
- `phase5_learning` (existing)
- `organ_coupling_learner` (existing)

**Phase B depends on:**
- Phase A (trained organism for testing)
- `sentence-transformers` (semantic similarity)
- `ConversationalOrganismWrapper` (test execution)

**Phase C depends on:**
- Phase A (training metrics)
- Phase B (test results)
- `matplotlib` / `seaborn` (visualization)

---

## Testing and Validation Strategy

### Unit Testing (Per Task)

**Each task includes unit tests:**
- A1: Test multi-iteration trainer (1 pair, 3 iterations)
- A2: Test epoch orchestrator integration (1 epoch, Râ‚…/Râ‚†/Râ‚‡)
- A3: Test training runner (1 epoch, corpus scaffolding)
- B1-B4: Test each test suite (smoke tests)
- C1-C4: Test metrics collection, visualization, reports

### Integration Testing (Per Phase)

**Phase A Integration:**
- Run 5-epoch training (EXPLORING â†’ CONVERGING)
- Verify regime transitions
- Verify Râ‚…, Râ‚†, Râ‚‡ computed

**Phase B Integration:**
- Run baseline tests (epoch 0)
- Run periodic tests (epoch 5)
- Verify all tests return structured results

**Phase C Integration:**
- Collect metrics from 15-epoch training
- Generate dashboard
- Generate report

### End-to-End Testing (Full System)

**Complete Workflow:**
1. Run 15-epoch training with metrics collection
2. Run comprehensive test suite (27 tests)
3. Generate visualization dashboard
4. Generate analysis report
5. Verify system maturity level â‰¥ Operational

---

## Risk Mitigation Plan

### Risk 1: Multi-Iteration Training Slows Performance

**Mitigation:**
- Profile processing time per iteration
- Set max iterations cap (5)
- Add early convergence termination
- If time > 1s avg, reduce iterations

### Risk 2: Tests Fail to Pass Thresholds

**Mitigation:**
- Lower thresholds initially (50% success)
- Gradually increase as system improves
- Document expected failure modes
- Provide remediation strategies

### Risk 3: Metrics Collection Overhead

**Mitigation:**
- Batch metric writes (every 10 epochs)
- Use lightweight data structures
- Async metric saving (non-blocking)

### Risk 4: Visualization Rendering Issues

**Mitigation:**
- Fallback to static plots (PNG) if HTML fails
- Test with different browsers
- Provide command-line report option

---

## Success Metrics and Checkpoints

### Week 1 Checkpoint

**Deliverables:**
- [ ] Multi-iteration trainer implemented (A1)
- [ ] Epoch orchestrator integration complete (A2)

**Success Criteria:**
- [ ] 1-epoch pilot runs successfully (30 pairs, 2-3 iterations)
- [ ] Râ‚…, Râ‚†, Râ‚‡ computed and saved

### Week 2 Checkpoint

**Deliverables:**
- [ ] Training runner script complete (A3)
- [ ] Regime adaptation implemented (A4)

**Success Criteria:**
- [ ] 5-epoch test runs (EXPLORING â†’ CONVERGING transition)
- [ ] Regime adaptation working (tau increases)

### Week 3 Checkpoint

**Deliverables:**
- [ ] Intelligence test suite (B1)
- [ ] Continuity test suite (B2)

**Success Criteria:**
- [ ] Baseline tests run (epoch 0)
- [ ] â‰¥50% of tests return valid results (not errors)

### Week 4 Checkpoint

**Deliverables:**
- [ ] Responsiveness test suite (B3)
- [ ] Superject cycle test (B4)

**Success Criteria:**
- [ ] Full test suite runnable
- [ ] Test results saved to JSON

### Week 5 Checkpoint

**Deliverables:**
- [ ] Metrics collector (C1)
- [ ] Visualization dashboard (C2)

**Success Criteria:**
- [ ] Metrics auto-collected during 3-epoch test
- [ ] Dashboard generates plots

### Week 6 Checkpoint (Final)

**Deliverables:**
- [ ] Analysis reports (C3)
- [ ] Experiment tracking (C4)

**Success Criteria:**
- [ ] 15-epoch training complete
- [ ] Comprehensive report generated
- [ ] System maturity level â‰¥ Operational (70% tests pass)
- [ ] CAGR â‰¥ 30% (target: 40-60%)

---

## Conclusion

This 6-week roadmap transforms DAE_HYPHAE_1 from single-iteration training to a comprehensive epoch-based learning system with multi-dimensional testing.

**Key Achievements:**
1. **Epoch Training** - 10-15 epochs with regime adaptation, scaffolded difficulty
2. **Fractal Learning** - Levels 1-7 fully operational (micro â†’ global)
3. **4D Testing** - 27 tests across Intelligence, Continuity, Responsiveness, Superject
4. **Automated Infrastructure** - Metrics collection, visualization, analysis reports
5. **Process Philosophy Validation** - Xâ†’Yâ†’Zâ†’X' cycle empirically testable

**Expected Outcome (Week 6):**
- System maturity: Operational â†’ Excellent (70-85% tests pass)
- Intelligence level: Abstraction â†’ Transfer (cross-category generalization)
- CAGR: 40-60% per epoch (compound learning growth)
- Families: 10-15 organic archetypes discovered
- Production ready for real-world conversational deployment

**Next Steps:**
1. User approval of roadmap
2. Begin Phase A Week 1 (multi-iteration trainer)
3. Daily progress updates and issue tracking
4. Weekly checkpoint reviews and adjustments

---

**Document Status:** Implementation Roadmap Complete
**Timeline:** 6 weeks (42 days)
**Total Code Estimate:** ~5,800 lines
**Team Size:** 1 developer (full-time) or 2 developers (part-time)

ðŸŒ€ **"From production-ready to research-grade. Epoch training, comprehensive testing, and process philosophy validation in 6 weeks."** ðŸŒ€
