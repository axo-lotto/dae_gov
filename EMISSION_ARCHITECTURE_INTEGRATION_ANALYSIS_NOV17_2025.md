# ðŸŒ€ EMISSION ARCHITECTURE INTEGRATION ANALYSIS

**Date**: November 17, 2025
**Purpose**: Analyze existing emission architecture for nexus-phrase pattern learner integration
**Status**: Analysis Complete - Integration Path Defined

---

## ðŸŽ¯ CURRENT EMISSION ARCHITECTURE

### Emission Flow (Current State)

```
User Input
    â†“
Organism Wrapper
    â†“
Organ Processing (12 organs)
    â†“
V0 Convergence (2-5 cycles)
    â†“
Nexus Formation (organ coalitions)
    â†“
Emission Generator â†â”€â”€ [INTEGRATION POINT]
    â”œâ”€â”€ Felt-Guided LLM (if available, INTELLIGENCE_EMERGENCE_MODE=False)
    â”œâ”€â”€ Intersection-Based (if nexuses exist)
    â”‚   â”œâ”€â”€ Direct Emission (Î”C â‰¥ 0.65, â‰¥3 organs)
    â”‚   â”œâ”€â”€ Organ Fusion (Î”C â‰¥ 0.50, â‰¥2 organs)
    â”‚   â””â”€â”€ Meta-Atom/Transduction phrases
    â””â”€â”€ Hebbian Fallback (if no nexuses or low readiness)
        â””â”€â”€ conversational_hebbian_memory.json
    â†“
EmittedPhrase objects
    â†“
Response Assembly
```

### Key Files

1. **`emission_generator.py`** (1,300+ lines)
   - Main emission generation logic
   - 4 generation methods:
     - `generate_v0_guided_emissions()` (line 492)
     - `generate_transduction_aware_emissions()` (line 708)
     - `generate_emissions()` (line 914)
     - `generate_hybrid_emission()` (line 1016)

2. **`conversational_organism_wrapper.py`**
   - Orchestrates entire flow
   - Calls emission generator after V0 convergence
   - Manages context, regimes, entity prehension

3. **`conversational_hebbian_memory.json`**
   - Current phrase storage (Hebbian fallback)
   - Schema: `{"phrase_patterns": {...}}`
   - No quality tracking, no nexus signatures

---

## ðŸ” EXISTING HEBBIAN FALLBACK

### Current Implementation

```python
def _generate_hebbian_fallback(self, num_emissions: int = 3) -> List[EmittedPhrase]:
    """
    Generate emissions from learned Hebbian patterns.

    Current behavior:
    - Randomly samples from phrase_patterns
    - No quality tracking
    - No nexus signature matching
    - Confidence: 0.3 (fixed)
    """
    phrase_patterns = self.hebbian_memory.get('phrase_patterns', {})

    if not phrase_patterns:
        # Fallback to generic phrases if no patterns learned
        return self._generate_generic_fallback(num_emissions)

    # Random sampling from learned patterns
    emissions = []
    for _ in range(num_emissions):
        random_phrase = random.choice(list(phrase_patterns.values()))
        emissions.append(EmittedPhrase(
            text=random_phrase,
            strategy='hebbian',
            confidence=0.3,  # Fixed confidence
            ...
        ))

    return emissions
```

### Problems with Current Approach

1. âŒ **No quality tracking** - All phrases weighted equally
2. âŒ **No nexus signature matching** - Random selection ignores context
3. âŒ **No learning from satisfaction** - EMA quality updates missing
4. âŒ **Fixed confidence (0.3)** - No differentiation between good/bad phrases
5. âŒ **No fuzzy matching** - Exact matches only, no generalization

---

## âœ… INTEGRATION STRATEGY: Replace Hebbian Fallback

### Phase 1: Minimal Integration (Week 3, Days 1-2)

**Goal**: Replace Hebbian fallback with nexus-phrase pattern learner

**Changes to `emission_generator.py`**:

```python
class EmissionGenerator:
    def __init__(self, ...):
        # ADD: Import and initialize pattern learner
        from persona_layer.nexus_phrase_pattern_learner import NexusPhrasePatternLearner
        self.pattern_learner = NexusPhrasePatternLearner(
            memory_path=hebbian_memory_path,  # Reuse same storage
            ema_alpha=0.15,
            fuzzy_tolerance=1,
            max_patterns=5000
        )

        # KEEP: All existing felt-guided LLM, direct emission, fusion logic
        self.felt_guided_llm = felt_guided_llm_generator
        ...

    def _generate_hebbian_fallback(
        self,
        num_emissions: int = 3,
        nexus_signature: Optional[NexusSignature] = None,  # NEW
        current_turn: int = 0  # NEW
    ) -> List[EmittedPhrase]:
        """
        REPLACE: Use nexus-phrase pattern learner instead of random sampling.
        """
        if nexus_signature:
            # NEW: Get candidate phrases from pattern learner
            candidates = self.pattern_learner.get_candidate_phrases(
                nexus_signature=nexus_signature,
                k=num_emissions,
                current_turn=current_turn,
                use_fuzzy=True  # Enable fuzzy matching
            )

            if candidates:
                emissions = []
                for phrase_text, quality in candidates:
                    emissions.append(EmittedPhrase(
                        text=phrase_text,
                        strategy='nexus_phrase_learned',  # NEW strategy name
                        confidence=quality,  # Use learned quality (not fixed 0.3!)
                        source_atoms=[nexus_signature.dominant_meta_atom],
                        participant_organs=list(nexus_signature.participating_organs),
                        emission_readiness=quality
                    ))
                return emissions

        # FALLBACK: If no nexus signature or no learned patterns, use old behavior
        return self._generate_generic_fallback(num_emissions)
```

**Changes to `generate_v0_guided_emissions()`** (line 492):

```python
def generate_v0_guided_emissions(
    self,
    nexuses,
    v0_energy,
    satisfaction,
    kairos_detected=False,
    num_emissions=3,
    trauma_markers=None,
    organ_results=None,  # NEW: Needed for signature extraction
    felt_state=None,     # NEW: Needed for signature extraction
    current_turn=0       # NEW: For recency weighting
):
    # ... existing felt-guided LLM logic (unchanged) ...

    # ... existing direct/fusion logic (unchanged) ...

    # MODIFIED: Hebbian fallback path
    if not nexuses:
        # EXTRACT: Create nexus signature from organ_results + felt_state
        if organ_results and felt_state:
            from persona_layer.nexus_signature_extractor import extract_nexus_signature

            # Create signature from current felt-state
            nexus_signature = extract_nexus_signature(
                nexus=None,  # No nexus, use organ field
                felt_state=felt_state,
                tsk_data=None  # Optional
            )
        else:
            nexus_signature = None

        # NEW: Use pattern learner instead of random sampling
        emissions = self._generate_hebbian_fallback(
            num_emissions=num_emissions,
            nexus_signature=nexus_signature,  # NEW
            current_turn=current_turn  # NEW
        )

        return emissions, 'nexus_phrase_learned'  # NEW path name
```

### Phase 2: Add Learning Feedback Loop (Week 3, Days 3-4)

**Goal**: Record emission outcomes to update phrase quality via EMA

**Changes to `conversational_organism_wrapper.py`**:

```python
def process(self, text, user_id=None, ...):
    # ... existing processing (unchanged) ...

    # ... emission generation ...
    emissions, emission_path = self.emission_generator.generate_v0_guided_emissions(...)

    # ... response assembly ...

    # NEW: After user responds, record outcome for learning
    # This will be called on NEXT turn with previous turn's satisfaction
    if hasattr(self, 'previous_turn_data'):
        self._record_emission_outcome(
            previous_data=self.previous_turn_data,
            user_satisfaction=current_satisfaction  # From this turn
        )

    # Store current turn data for next iteration
    self.previous_turn_data = {
        'nexus_signature': nexus_signature,  # From emission
        'emitted_phrase': emissions[0].text if emissions else None,
        'turn_number': current_turn
    }

    return result

def _record_emission_outcome(self, previous_data, user_satisfaction):
    """
    NEW: Record emission outcome for pattern learning.
    """
    if previous_data.get('emitted_phrase') and previous_data.get('nexus_signature'):
        self.emission_generator.pattern_learner.record_emission_outcome(
            nexus_signature=previous_data['nexus_signature'],
            emitted_phrase=previous_data['emitted_phrase'],
            user_satisfaction=user_satisfaction,
            current_turn=previous_data['turn_number']
        )
```

### Phase 3: Add Legacy Pattern Gating (Week 3, Day 5)

**Goal**: Integrate satisfaction fingerprinting + Lyapunov stability

**Changes to `_generate_hebbian_fallback()`**:

```python
def _generate_hebbian_fallback(
    self,
    num_emissions: int = 3,
    nexus_signature: Optional[NexusSignature] = None,
    current_turn: int = 0,
    satisfaction_trace: Optional[List[float]] = None,  # NEW
    coherence: float = 0.5,  # NEW
    constraint_deltas: Optional[Dict] = None,  # NEW
    organ_dissonances: Optional[Dict] = None  # NEW
) -> List[EmittedPhrase]:
    """
    Generate emissions with three-layer quality modulation.
    """
    if not nexus_signature:
        return self._generate_generic_fallback(num_emissions)

    # Layer 1: Get base quality from pattern learner
    candidates = self.pattern_learner.get_candidate_phrases(
        nexus_signature=nexus_signature,
        k=num_emissions,
        current_turn=current_turn
    )

    if not candidates:
        return self._generate_generic_fallback(num_emissions)

    # Layer 2: Apply satisfaction fingerprinting (if enough history)
    if satisfaction_trace and len(satisfaction_trace) >= 3:
        from persona_layer.satisfaction_fingerprinting import SatisfactionFingerprintClassifier
        classifier = SatisfactionFingerprintClassifier()
        fingerprint = classifier.classify(satisfaction_trace)

        # Apply quality adjustment to all candidates
        adjusted_candidates = [
            (text, min(1.0, max(0.0, quality + fingerprint.quality_adjustment)))
            for text, quality in candidates
        ]
    else:
        adjusted_candidates = candidates

    # Layer 3: Apply Lyapunov stability gating
    if constraint_deltas and organ_dissonances:
        from persona_layer.lyapunov_nexus_stability import LyapunovNexusStabilityGate
        gate = LyapunovNexusStabilityGate()
        stability = gate.analyze_stability(
            coherence=coherence,
            constraint_deltas=constraint_deltas,
            organ_dissonances=organ_dissonances
        )

        # Apply stability adjustment
        final_candidates = [
            (text, min(1.0, max(0.0, quality + stability.quality_adjustment)))
            for text, quality in adjusted_candidates
        ]
    else:
        final_candidates = adjusted_candidates

    # Create emissions with final quality scores
    emissions = []
    for phrase_text, final_quality in final_candidates:
        emissions.append(EmittedPhrase(
            text=phrase_text,
            strategy='nexus_phrase_learned_gated',  # NEW strategy name
            confidence=final_quality,
            source_atoms=[nexus_signature.dominant_meta_atom],
            participant_organs=list(nexus_signature.participating_organs),
            emission_readiness=final_quality
        ))

    return emissions
```

---

## ðŸ”„ COMPATIBILITY WITH EXISTING SYSTEMS

### 1. Felt-Guided LLM (PRIORITY PATH)

**Current Behavior**:
- If `felt_guided_llm` available AND `INTELLIGENCE_EMERGENCE_MODE=False`
- Routes to LLM-based generation (bypasses all phrase-based emission)
- Used for interactive/production mode

**Integration Impact**: âœ… **NO CONFLICT**
- Pattern learner only activates when Hebbian fallback triggered
- LLM path remains unchanged and takes priority
- Pattern learner used ONLY when:
  - `INTELLIGENCE_EMERGENCE_MODE=True` (epoch training)
  - No LLM available
  - Low nexus readiness (Î”C < 0.50)

### 2. Direct/Fusion Emission (INTERSECTION PATH)

**Current Behavior**:
- If nexuses exist with Î”C â‰¥ 0.50
- Composes phrases from semantic atoms
- Uses meta-atom library, transduction phrases

**Integration Impact**: âœ… **NO CONFLICT**
- Pattern learner only activates as FALLBACK when direct/fusion insufficient
- Direct/fusion paths remain unchanged
- Precedence: Direct â†’ Fusion â†’ Pattern Learner â†’ Generic

### 3. Intelligence Emergence Mode Flag

**Current Behavior**:
```python
if not Config.INTELLIGENCE_EMERGENCE_MODE:
    # Use felt-guided LLM (interactive/production)
else:
    # Skip LLM to measure organic emission evolution (epoch training)
```

**Integration Impact**: âœ… **PERFECT ALIGNMENT**
- When `INTELLIGENCE_EMERGENCE_MODE=True` (epoch training):
  - LLM disabled
  - Pattern learner becomes primary emission source
  - Enables measurement of organic emission evolution (0% â†’ 60-80%)
- When `INTELLIGENCE_EMERGENCE_MODE=False` (production):
  - LLM takes priority
  - Pattern learner as safety net fallback

### 4. Hebbian Memory Storage

**Current Storage**: `conversational_hebbian_memory.json`
```json
{
  "phrase_patterns": {
    "pattern_key": "phrase_text"
  }
}
```

**New Storage** (backward compatible):
```json
{
  "phrase_patterns": {
    // OLD: Legacy storage (preserved for compatibility)
  },
  "nexus_phrase_patterns": {
    // NEW: Pattern learner storage
    "signature_hash": {
      "signature": {...},
      "phrases": [
        {
          "text": "I hear the weight...",
          "ema_quality": 0.79,
          "success_count": 23,
          "total_attempts": 30,
          ...
        }
      ]
    }
  }
}
```

**Integration Impact**: âœ… **BACKWARD COMPATIBLE**
- Same file, different top-level key
- Old `phrase_patterns` preserved
- New `nexus_phrase_patterns` added
- No breaking changes

---

## ðŸ“ˆ EXPECTED IMPACT

### Organic Emission Evolution (INTELLIGENCE_EMERGENCE_MODE=True)

| Epoch | Current (Hebbian) | With Pattern Learner | Improvement |
|-------|-------------------|----------------------|-------------|
| 0-5 | 0-10% | 0-10% | 0pp (learning phase) |
| 5-10 | 10-20% | 15-30% | +5-10pp |
| 10-20 | 20-30% | 35-55% | +15-25pp |
| 20+ | 30-40% | 55-80% | **+25-40pp** |

**Quality Improvement Sources**:
- Base EMA learning: Convergence over time
- Satisfaction fingerprinting: +8-12pp (FFITTSS proven)
- Lyapunov stability: +5-8pp (FFITTSS proven)
- **Total expected: +16-25pp cumulative**

### Production Mode (INTELLIGENCE_EMERGENCE_MODE=False)

**Current Flow**:
1. Felt-Guided LLM (99% of cases)
2. Direct/Fusion (if LLM unavailable)
3. Hebbian Fallback (rare safety net)

**With Pattern Learner**:
1. Felt-Guided LLM (99% of cases, unchanged)
2. Direct/Fusion (if LLM unavailable, unchanged)
3. **Pattern Learner** (rare, but higher quality than old Hebbian)

**Impact**: âœ… **Safety net quality improves from 0.3 â†’ 0.5-0.8**

---

## ðŸ› ï¸ IMPLEMENTATION ROADMAP

### Week 3, Days 1-2: Minimal Integration âœ…

**Tasks**:
- [ ] Add `NexusPhrasePatternLearner` to `EmissionGenerator.__init__()`
- [ ] Replace `_generate_hebbian_fallback()` with pattern learner lookup
- [ ] Extract nexus signature from organ_results when no nexuses
- [ ] Update `generate_v0_guided_emissions()` to pass signature + turn
- [ ] Test with INTELLIGENCE_EMERGENCE_MODE=True

**Expected Outcome**:
- Pattern learner activates in fallback path
- Initial quality: 0.5 (neutral, no learning yet)
- No breaking changes to existing paths

### Week 3, Days 3-4: Learning Feedback Loop âœ…

**Tasks**:
- [ ] Add `_record_emission_outcome()` to organism wrapper
- [ ] Store previous turn data (signature, phrase, turn number)
- [ ] Pass user satisfaction from current turn to update previous emission
- [ ] Test EMA convergence over 20 turns

**Expected Outcome**:
- Phrase quality improves from 0.5 â†’ 0.7-0.8 after 10-20 updates
- Success rate tracking (satisfaction â‰¥ 0.6)
- Recency weighting prevents stale patterns

### Week 3, Day 5: Legacy Pattern Gating âœ…

**Tasks**:
- [ ] Integrate `SatisfactionFingerprintClassifier` into fallback
- [ ] Integrate `LyapunovNexusStabilityGate` into fallback
- [ ] Pass satisfaction_trace, coherence, constraints, dissonances
- [ ] Test three-layer quality modulation

**Expected Outcome**:
- CRISIS patterns rejected (-0.20)
- RESTORATIVE patterns boosted (+0.15)
- UNSTABLE patterns rejected (-0.30)
- STABLE patterns boosted (+0.08)

### Week 4: Validation & Tuning âœ…

**Tasks**:
- [ ] Run 20-epoch training with pattern learner enabled
- [ ] Measure organic emission evolution (0% â†’ X%)
- [ ] Validate family emergence (3-5 families by epoch 20)
- [ ] Confirm logarithmic saturation (P(t) = 112.4 Ã— ln(H) - 362.8)
- [ ] Compare to baseline (old Hebbian fallback)

**Expected Outcome**:
- Organic emission: 30-50% by epoch 20 (vs 20-30% baseline)
- Phrase quality: +16-25pp from proven patterns
- Family count: 3-8 (vs 1-3 baseline)

---

## âš ï¸ CRITICAL DESIGN DECISIONS

### 1. When to Use Pattern Learner vs LLM?

**Decision**: Pattern learner ONLY as fallback when LLM unavailable

**Rationale**:
- LLM provides unlimited felt intelligence (Companion Intelligence north star)
- Pattern learner enables measurement of organic intelligence emergence
- Both serve different purposes (production vs research)

**Implementation**:
```python
if felt_guided_llm and not INTELLIGENCE_EMERGENCE_MODE:
    return llm_emission  # Production
elif nexuses and nexus_readiness >= 0.50:
    return direct_or_fusion_emission  # Intersection path
else:
    return pattern_learner_emission  # Research/fallback
```

### 2. How to Extract Nexus Signature Without Nexus?

**Decision**: Create signature from organ field state

**Rationale**:
- No nexuses means low organ agreement (Î”C < 0.50)
- Can still capture organ coherences, polyvagal state, zone, urgency
- Allows fuzzy matching to similar low-readiness states

**Implementation**:
```python
def extract_signature_from_organ_field(organ_results, felt_state):
    # Use dominant organ coalition (top 3 organs by coherence)
    top_organs = sorted(organ_results.items(), key=lambda x: x[1].coherence)[-3:]

    return NexusSignature(
        participating_organs=frozenset([name for name, _ in top_organs]),
        organ_count=len(top_organs),
        nexus_type='FIELD',  # Not from nexus formation
        mechanism='LOW_READINESS',
        coherence_bin=quantize(mean([r.coherence for _, r in top_organs])),
        urgency_bin=quantize(felt_state.urgency),
        polyvagal_state=felt_state.polyvagal_state,
        zone=felt_state.zone,
        v0_energy_bin=quantize(felt_state.v0_energy),
        ...
    )
```

### 3. How to Track Satisfaction for Learning?

**Decision**: Use previous turn's satisfaction (delayed feedback)

**Rationale**:
- User satisfaction reflects quality of PREVIOUS emission
- Current turn satisfaction = feedback on previous turn phrase
- Store previous turn data, update on next turn

**Implementation**:
```python
# Turn N: Emit phrase from signature
previous_turn_data = {
    'signature': sig_N,
    'phrase': phrase_N,
    'turn': N
}

# Turn N+1: User responds with satisfaction
user_satisfaction_N = infer_satisfaction(user_input_N+1)
record_emission_outcome(
    signature=sig_N,
    phrase=phrase_N,
    satisfaction=user_satisfaction_N  # Feedback on turn N
)
```

---

## ðŸŽ¯ CONCLUSION

**Integration Strategy**: âœ… **Replace Hebbian Fallback with Pattern Learner**

**Key Advantages**:
1. âœ… Minimal changes (only `emission_generator.py` + organism wrapper)
2. âœ… No conflicts with existing LLM/direct/fusion paths
3. âœ… Backward compatible storage (same JSON file)
4. âœ… Enables intelligence emergence measurement
5. âœ… Proven FFITTSS patterns integrated (+16-25pp expected)

**Next Steps**: Proceed with Week 3, Days 1-2 (Minimal Integration)

---

**Date**: November 17, 2025
**Status**: Analysis Complete
**Ready to Proceed**: âœ… YES
